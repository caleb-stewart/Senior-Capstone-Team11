{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f6ec8e-fc7e-48fd-ae2c-84b501882d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # install al lthis stuff\n",
    "# !pip install opencv-python\n",
    "# !pip install ultralytics\n",
    "# !pip install pillow\n",
    "# !pip install faiss-cpu\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1aeac4-b821-430e-9f70-d7f45be7e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BeitImageProcessor, BeitModel\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de55201-02fb-4df8-8615-5d700e391a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEiT Embedding because this article used it:\n",
    "# https://medium.com/@nathanjjacob/how-to-build-a-logo-detection-recognition-system-at-scale-c2b094ae4fd2\n",
    "\n",
    "# This basically turns an image into a vector\n",
    "class BEiTEmbedding:\n",
    "    def __init__(self, model_name=\"microsoft/beit-base-patch16-224\"):\n",
    "        # Load the BEiT model from Hugging Face\n",
    "        self.feature_extractor = BeitImageProcessor.from_pretrained(model_name)  # Corrected class name\n",
    "        self.model = BeitModel.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Use average pooling\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class CLIPEmbedding:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.get_image_features(**inputs)\n",
    "        return outputs.squeeze().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195dfa54-ff27-494c-8044-8e45229a577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "    # this function is from sklearn. Thank you sklearn :)\n",
    "    # this does the math for us\n",
    "    return cosine_similarity(embedding1, embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "714459ad-bc6a-46c9-a547-b046551709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logo_regions(image_path, model):\n",
    "    # Returns all of the logos found within an image\n",
    "\n",
    "    img = cv2.imread(image_path)  # Read the image here\n",
    "    results = model(img)\n",
    "    logo_regions = []\n",
    "    bounding_boxes = []\n",
    "    counter = 1\n",
    "    for box in results[0].boxes:\n",
    "        xyxy = box.xyxy[0].tolist()\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        cropped_logo = img[y1:y2, x1:x2]  # Extract detected region\n",
    "\n",
    "        if cropped_logo.size > 0:\n",
    "            \n",
    "            height, width = cropped_logo.shape[:2]\n",
    "\n",
    "            # Calculate new width while maintaining aspect ratio\n",
    "            new_height = 128\n",
    "            new_width = int((new_height / height) * width)\n",
    "            \n",
    "            # Resize while keeping aspect ratio\n",
    "            resized_logo = cv2.resize(cropped_logo, (new_width, new_height))\n",
    "\n",
    "            cv2.imwrite(f'./cropped/cropped_{counter}_{image_path}', resized_logo)\n",
    "            counter += 1\n",
    "            # Convert grayscale to 3-channel (RGB-like) image\n",
    "            # three_channel_logo = cv2.cvtColor(resized_logo, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            logo_regions.append(resized_logo)\n",
    "            bounding_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "    return logo_regions, bounding_boxes, img  # Return img here\n",
    "\n",
    "def compare_logo_embeddings(input_path, reference_path, model, feature_extractor, similarity_threshold=0.0):\n",
    "    #compare logos using embeddings\n",
    "    \n",
    "    # Extract logos and bounding boxes, also get the image\n",
    "    input_logos, input_bboxes, input_img = extract_logo_regions(input_path, model)\n",
    "    reference_logos, reference_bboxes, reference_img = extract_logo_regions(reference_path, model)\n",
    "    \n",
    "    if not input_logos or not reference_logos:\n",
    "        print(\"No logos detected in one or both images.\")\n",
    "        return\n",
    "    \n",
    "    # get embeddings (vectors) from the reference and input image\n",
    "    input_embeddings = [feature_extractor.extract_embedding(Image.fromarray(input_logo)) for input_logo in input_logos]\n",
    "    reference_embeddings = [feature_extractor.extract_embedding(Image.fromarray(reference_logo)) for reference_logo in reference_logos]\n",
    "    \n",
    "    # Compare logos using cosine similarity\n",
    "    for index, input_embedding in enumerate(input_embeddings):\n",
    "        for ref_index, reference_embedding in enumerate(reference_embeddings):\n",
    "            similarity = compute_cosine_similarity(input_embedding, reference_embedding)\n",
    "\n",
    "            print(f'similarity score: {similarity}')\n",
    "            # If similarity is above the threshold, then they match\n",
    "            if similarity >= similarity_threshold:\n",
    "                # Get the bounding box for the matching input logo\n",
    "                x1, y1, x2, y2 = input_bboxes[index]\n",
    "                color = [255, 255, 255]\n",
    "                cv2.rectangle(input_img, (x1, y1), (x2, y2), color, 2)  # Draw on the input image\n",
    "\n",
    "    # Save the processed image after drawing the rectangles\n",
    "    cv2.imwrite(\"output_image.jpg\", input_img)\n",
    "    print(\"Processed image saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d52c4c-b5c5-4792-a151-df3a5538c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 3 logos, 20.2ms\n",
      "Speed: 1.2ms preprocess, 20.2ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 logo, 17.0ms\n",
      "Speed: 0.9ms preprocess, 17.0ms inference, 0.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "similarity score: [[    0.10802]]\n",
      "similarity score: [[    0.13226]]\n",
      "similarity score: [[    0.46391]]\n",
      "Processed image saved.\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"starbucks.jpg\"  # Change this to your image file\n",
    "reference_image_path = \"reference.jpg\"\n",
    "yolo_model = YOLO(\"../488_back/best.pt\")  # Load your YOLO model\n",
    "compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, BEiTEmbedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aaa3d-fbfe-4870-96f0-254bc3f749a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
